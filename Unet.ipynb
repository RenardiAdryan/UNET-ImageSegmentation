{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch import nn, cat,rand\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Datasets into two part which is train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_images_dir = \"data/raw_data/raw_images\"\n",
    "raw_data_mask_dir = \"data/raw_data/raw_masks\"\n",
    "\n",
    "dest_train_images_dir = \"data/train/train_images\"\n",
    "dest_train_masks_dir = \"data/train/train_masks\"\n",
    "dest_val_images_dir = \"data/val/val_images\"\n",
    "dest_val_masks_dir = \"data/val/val_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "data={\"images\": os.listdir(raw_data_images_dir),\n",
    "\"masks\" : os.listdir(raw_data_mask_dir)\n",
    "}\n",
    "images_data_dir_df = pd.DataFrame(data)\n",
    "\n",
    "# Split Datasets\n",
    "train_size = 0.8\n",
    "train_dataset=images_data_dir_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=images_data_dir_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy to desired folder\n",
    "def copy_data(src_images_dir,src_images_name,dest_images_dir):\n",
    "    shutil.copy(os.path.join(src_images_dir,src_images_name),dest_images_dir)\n",
    "\n",
    "#Copy Train Images\n",
    "for images in train_dataset['images']:\n",
    "    copy_data(raw_data_images_dir,images,dest_train_images_dir)\n",
    "\n",
    "#Copy Train Masks\n",
    "for images in train_dataset['masks']:\n",
    "    copy_data(raw_data_mask_dir,images,dest_train_masks_dir)\n",
    "\n",
    "#Copy val Images\n",
    "for images in test_dataset['images']:\n",
    "    copy_data(raw_data_images_dir,images,dest_val_images_dir)\n",
    "\n",
    "#Copy val Masks\n",
    "for images in test_dataset['masks']:\n",
    "    copy_data(raw_data_mask_dir,images,dest_val_masks_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3\n",
    "NUM_WORKERS = 0\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = dest_train_images_dir\n",
    "TRAIN_MASK_DIR = dest_train_masks_dir\n",
    "VAL_IMG_DIR = dest_val_images_dir\n",
    "VAL_MASK_DIR = dest_val_masks_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir,mask_dir,transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir  = mask_dir\n",
    "        self.transfrom = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_path = os.path.join(self.image_dir,self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir,self.images[index].replace(\".jpg\",\"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"),dtype=np.float32)\n",
    "        mask[mask==255.0] = 1.0\n",
    "\n",
    "        if self.transfrom is not None:\n",
    "            augmentations = self.transfrom(image=image,mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "        \n",
    "        return image,mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "train_transform = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Rotate(limit=35, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.1),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "            A.Normalize(\n",
    "                mean=[0.0, 0.0, 0.0],\n",
    "                std=[1.0, 1.0, 1.0],\n",
    "                max_pixel_value=255.0,\n",
    "            ),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': NUM_WORKERS\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': NUM_WORKERS\n",
    "                }\n",
    "\n",
    "training_set = CarvanaDataset(TRAIN_IMG_DIR,TRAIN_MASK_DIR,transform=train_transform)\n",
    "testing_set = CarvanaDataset(VAL_IMG_DIR,VAL_MASK_DIR,transform=val_transforms)\n",
    "\n",
    "training_loader = DataLoader(dataset = training_set, **train_params)\n",
    "testing_loader = DataLoader(dataset = testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:  torch.Size([16, 3, 160, 240])\n",
      "Masks:  torch.Size([16, 160, 240])\n"
     ]
    }
   ],
   "source": [
    "data=next(iter(testing_loader))\n",
    "print(\"Images: \",data[0].shape)\n",
    "print(\"Masks: \",data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (downs): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "model = UNET(in_channels=3, out_channels=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from torch import nn, cat\n",
    "\n",
    "\n",
    "#VGG Block, baca: https://d2l.ai/chapter_convolutional-modern/vgg.html\n",
    "class conv_block(nn.Module):\n",
    "\tdef __init__(self, in_channels, mid_channels, out_channels):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.relu = nn.ReLU(inplace=True) # https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\t\t\n",
    "\t\t#LAYER KONVOLUSI 1\n",
    "\t\t#kernel filter konvolusi 3x3\n",
    "\t\t#stride pergeseran kernel tiap 1 pixel\n",
    "\t\t#tambah padding 1 untuk H dan W, padding = 0\n",
    "\t\t#sehingga output H x W tidak berubah setelah proses convolusi\n",
    "\t\t#mid channel sbg jml output channel conv pertama\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3,\n",
    "\t\t\t\t\tstride=1, padding=1, padding_mode='zeros')\n",
    "\t\tself.bn1 = nn.BatchNorm2d(mid_channels) #normalisasi dari sekian channel\n",
    "\t\t\n",
    "\t\t#LAYER KONVOLUSI 2\n",
    "\t\t#kernel filter konvolusi 3x3\n",
    "\t\t#stride pergeseran kernel tiap 1 pixel\n",
    "\t\t#tambah padding 1 untuk H dan W, padding = 0\n",
    "\t\t#sehingga output H x W tidak berubah setelah proses convolusi\n",
    "\t\t#mid channel sbg jml input channel conv kedua\n",
    "\t\tself.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3,\n",
    "\t\t\t\t\tstride=1, padding=1, padding_mode='zeros')\n",
    "\t\tself.bn2 = nn.BatchNorm2d(out_channels) #normalisasi dari sekian channel\n",
    "\n",
    "\t\t#LAYER KONVOLUSI 3\n",
    "\t\t#kernel filter konvolusi 3x3\n",
    "\t\t#stride pergeseran kernel tiap 1 pixel\n",
    "\t\t#tambah padding 1 untuk H dan W, padding = 0\n",
    "\t\t#sehingga output H x W tidak berubah setelah proses convolusi\n",
    "\t\t#mid channel sbg jml input channel conv kedua\n",
    "\t\t#self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=3,\n",
    "\t\t#\t\t\tstride=1, padding=1, padding_mode='zeros')\n",
    "\t\t#self.bn3 = nn.BatchNorm2d(out_channels) #normalisasi dari sekian channel\n",
    "\n",
    "\t#1 block berisi conv, batch normalisasi, dan aktivasi relu sebanyak 3x\n",
    "\tdef forward(self, x): #x adalah tensor yang dimasukkan\n",
    "\t\tx = self.conv1(x) #konvolusi\n",
    "\t\tx = self.bn1(x) #normalisasi\n",
    "\t\tx = self.relu(x) #aktivasi\n",
    "\t\tx = self.conv2(x) #konvolusi\n",
    "\t\tx = self.bn2(x) #normalisasi\n",
    "\t\tx = self.relu(x) #aktivasi\n",
    "\t\t#x = self.conv3(x) #konvolusi\n",
    "\t\t#x = self.bn3(x) #normalisasi\n",
    "\t\t#x = self.relu(x) #aktivasi\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "#baca: https://arxiv.org/pdf/1505.04597.pdf\n",
    "class UNet(nn.Module): \n",
    "\t#default input channel adalah 3, asumsi pembacaan cv2 adalah RGB\n",
    "\tdef __init__(self, n_class, **kwargs):\n",
    "\t\tsuper().__init__()\n",
    "\t\t#jumlah channel feature map yang dikehendaki\n",
    "\t\t#alias jumlah kernel konvolusi pada setiap layernya\n",
    "\t\tn_fmap_ch = [32, 64, 128, 256, 512] \n",
    "\t\t\n",
    "\t\t\t\n",
    "\t\t#fungsi downsampling (dengan maxpooling) dan upsampling\n",
    "\t\t#kernel pooling HxW = 2x2, no padding dan\n",
    "\t\t#stride=2 sehingga dimensi HxW ter-downsampling menjadi H/2 x W/2\n",
    "\t\t#max pooling, berarti dari 2x2 kotak pixel diambil yang paling besar (max)\n",
    "\t\tself.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) \n",
    "\t\t#spatial (HxW) size dikali oleh scale_factor=2\n",
    "\t\t#sehingga HxW ter-upsampling menjadi H*2 x W*2\n",
    "\t\tself.up = nn.Upsample(scale_factor=2)\n",
    "\t\t\n",
    "\t\t#bagian downsampling\n",
    "\t\t# format seperti Block di atas: in_channels, mid_channels, out_channels\n",
    "\t\t# isi block tergantung conv block yang dipakai, lihat di atas\n",
    "\t\tself.conv0_0 = conv_block(3, n_fmap_ch[0], n_fmap_ch[0])\n",
    "\t\tself.conv1_0 = conv_block(n_fmap_ch[0], n_fmap_ch[1], n_fmap_ch[1])\n",
    "\t\tself.conv2_0 = conv_block(n_fmap_ch[1], n_fmap_ch[2], n_fmap_ch[2])\n",
    "\t\tself.conv3_0 = conv_block(n_fmap_ch[2], n_fmap_ch[3], n_fmap_ch[3])\n",
    "\t\t#bagian neck\n",
    "\t\tself.conv4_0 = conv_block(n_fmap_ch[3], n_fmap_ch[4], n_fmap_ch[4])\n",
    "\t\t\n",
    "\t\t#bagian upsampling\n",
    "\t\t#jumlahkan channel output layer sebelumnya dengan channel output pada downsampling yang sesuai\n",
    "\t\t# isi block tergantung conv block yang dipakai, lihat di atas\n",
    "\t\tself.conv3_1 = conv_block(n_fmap_ch[3]+n_fmap_ch[4], n_fmap_ch[3], n_fmap_ch[3])\n",
    "\t\tself.conv2_1 = conv_block(n_fmap_ch[2]+n_fmap_ch[3], n_fmap_ch[2], n_fmap_ch[2])\n",
    "\t\tself.conv1_1 = conv_block(n_fmap_ch[1]+n_fmap_ch[2], n_fmap_ch[1], n_fmap_ch[1])\n",
    "\t\tself.conv0_1 = conv_block(n_fmap_ch[0]+n_fmap_ch[1], n_fmap_ch[0], n_fmap_ch[0])\n",
    "\t\t\n",
    "\t\t#n_class sebagai channel output akhir\n",
    "\t\t#1 x konvolusi yang menghasilkan sejumlah n_class output feature map\n",
    "\t\t#ukuran kernel konvolusi 1x1, sehingga ukuran HxW tidak berubah\n",
    "\t\tself.final = nn.Conv2d(n_fmap_ch[0], n_class, kernel_size=1)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\t#perhatikan n_fmap_ch = [32, 64, 128, 256, 512] di atas\n",
    "\t\t#bagian downsampling\n",
    "\t\tx0_0 = self.conv0_0(input) #in_ch=RGB=3, out_ch=32, HxW=inputHxW=64x128 \n",
    "\t\tdown_x0_0 = self.pool(x0_0) #HxW=(64x128)/2=32x64\n",
    "\t\tx1_0 = self.conv1_0(down_x0_0)#in_ch=32, out_ch=64\n",
    "\t\tdown_x1_0 = self.pool(x1_0) #HxW=(32x64)/2=16x32\n",
    "\t\tx2_0 = self.conv2_0(down_x1_0)#in_ch=64, out_ch=128\n",
    "\t\tdown_x2_0 = self.pool(x2_0) #HxW=(16x32)/2=8x16\n",
    "\t\tx3_0 = self.conv3_0(down_x2_0) #in_ch=128, out_ch=256\n",
    "\t\tdown_x3_0 = self.pool(x3_0) #HxW=(8x16)/2=4x8\n",
    "\t\t\n",
    "\t\t#bagian neck\n",
    "\t\tx4_0 = self.conv4_0(down_x3_0) #in_ch=256, out_ch=512\n",
    "\t\t\n",
    "\t\t#bagian upsampling\n",
    "\t\t#dan concatenate dengan setiap output di downsampling sebelumnya\n",
    "\t\t#concat pada axis dim 1, karena yang diconcate adalah channel feature mapnya\n",
    "\t\t#ingat! tensor dimension: batch x channel x H x W ->> 0,1,2,3, axis dim 1 adalah channel\n",
    "\t\tup_x4_0 = self.up(x4_0) #HxW=(4x8)*2=8x16\n",
    "\t\tx3_1 = self.conv3_1(cat([x3_0, up_x4_0], dim=1)) #in_ch=256+512=768, out_ch=256, \n",
    "\t\tup_x3_1 = self.up(x3_1) #HxW=(8x16)*2=16x32\n",
    "\t\tx2_1 = self.conv2_1(cat([x2_0, up_x3_1], dim=1)) #in_ch=128+256=384, out_ch=128, \n",
    "\t\tup_x2_1 = self.up(x2_1) #HxW=(16x32)*2=32x64\n",
    "\t\tx1_1 = self.conv1_1(cat([x1_0, up_x2_1], dim=1)) #in_ch=64+128=192, out_ch=64,\n",
    "\t\tup_x1_1 = self.up(x1_1) #HxW=(32x64)*2=64x128\n",
    "\t\tx0_1 = self.conv0_1(cat([x0_0, up_x1_1], dim=1)) #in_ch=32+64=96, out_ch=32,\n",
    "\t\t\n",
    "\t\t#perhatikan self_final,\n",
    "\t\t#jika hanya 1 class maka output UNet ini juga 1 lapis layer saja\n",
    "\t\t#ukuran kernel konvolusi 1x1, sehingga ukuran HxW tidak berubah\n",
    "\t\t#in_ch=32, out_ch=jumlah_class=1, HxW=128x256\n",
    "\t\toutput = self.final(x0_1)\n",
    "\t\t#output = nn.Sigmoid(output)\n",
    "\t\treturn output\n",
    "\n",
    "model = UNet(n_class=3)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pred Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 1, 161, 161))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "preds = model(x)\n",
    "assert preds.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_fn(loader,model, optimizer, loss_fn, scaler):\n",
    "    with tqdm(loader, unit=\"batch\") as tepoch:\n",
    "        for batch_idx, (data,targets) in enumerate(tepoch):\n",
    "            data = data.to(device=DEVICE)\n",
    "            targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
    "\n",
    "            # forward\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(data)\n",
    "                loss = loss_fn(predictions,targets)\n",
    "            \n",
    "            #backward\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # update tqdm loop\n",
    "            tepoch.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Engineering\\anaconda3\\envs\\ssd-pytorch\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# if LOAD_MODEL:\n",
    "    # load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model,device):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "\n",
    "            preds = model((x))\n",
    "            preds = torch.sigmoid(preds)\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct +=(preds==y).sum()\n",
    "\n",
    "            num_pixels += torch.numel(preds)\n",
    "\n",
    "            dice_score += (2 * (preds * y).sum()) / (\n",
    "                (preds + y).sum() + 1e-8\n",
    "            )\n",
    "        print(\n",
    "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "def save_predictions_as_imgs(\n",
    "    loader, model, folder=\"saved_images/\", device=\"cuda\"\n",
    "):\n",
    "    model.eval()\n",
    "    for idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device=device)\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "        torchvision.utils.save_image(\n",
    "            preds, f\"{folder}/pred_{idx}.png\"\n",
    "        )\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/255 [00:00<?, ?batch/s]C:\\Users\\Engineering\\anaconda3\\envs\\ssd-pytorch\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "  2%|▏         | 5/255 [02:00<1:40:20, 24.08s/batch, loss=0.546]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      8\u001b[0m     }\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(loader, model, optimizer, loss_fn, scaler)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#backward\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     17\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssd-pytorch\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ssd-pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(training_loader,model,optimizer,loss_fn,scaler)\n",
    "\n",
    "    # save model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\":optimizer.state_dict(),\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    # check accuracy\n",
    "    check_accuracy(testing_loader, model, device=DEVICE)\n",
    "\n",
    "    # print some examples to a folder\n",
    "    save_predictions_as_imgs(\n",
    "        testing_loader, model, folder=\"saved_images/\", device=DEVICE\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c22016eb7d79489731da8442ca81b77735177131bbe84d46e6e2f089c7fc626"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ssd-pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
